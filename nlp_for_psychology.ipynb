{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41cd272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/14/tgjmqxwd3mq_dmn00xtb25lw0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['所以', '我在每一个无法入睡的夜晚', '像小时候一样', '攥紧拳头', '捂紧被子不让哭泣的声音泄露在黑暗里', '等哭累了', '也就慢慢睡着了', '我怕招人厌恶', '我怕恶语相加', '我怕看到他不耐烦和厌恶的表情。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.327 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所以\n",
      "我, 在, 每, 一个, 无法, 入睡, 的, 夜晚\n",
      "像, 小时候, 一样\n",
      "攥, 紧, 拳头\n",
      "捂紧, 被子, 不让, 哭泣, 的, 声音, 泄露, 在, 黑暗, 里\n",
      "等, 哭, 累, 了\n",
      "也, 就, 慢慢, 睡着, 了\n",
      "我怕, 招人, 厌恶\n",
      "我怕, 恶语相加\n",
      "我怕, 看到, 他, 不耐烦, 和, 厌恶, 的, 表情, 。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "sample_str = \"所以，我在每一个无法入睡的夜晚，像小时候一样，攥紧拳头，\" \\\n",
    "             \"捂紧被子不让哭泣的声音泄露在黑暗里，等哭累了，也就慢慢睡着了，\" \\\n",
    "             \"我怕招人厌恶，我怕恶语相加，我怕看到他不耐烦和厌恶的表情。\"\n",
    "# 分词演示\n",
    "pre_process = sample_str.split('，')\n",
    "\n",
    "print(pre_process)\n",
    "for item in pre_process:\n",
    "    seg_list = jieba.cut(item)\n",
    "    print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0602233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "佛洛依德 ORG\n",
      "卡尔荣格 GPE\n",
      "2018年 DATE\n",
      "北京 GPE\n",
      "2020年3月28日 DATE\n",
      "重庆 GPE\n"
     ]
    }
   ],
   "source": [
    "# 命名实体识别演示\n",
    "import spacy\n",
    "\n",
    "sample2_str = \"最近在看佛洛依德、卡尔荣格的书，但是我不承认我心里的残缺面，在2018年患病的初期，我真的特别感激我的男朋友，\" \\\n",
    "              \"陪我去北京看病，容忍了一次又一次我情绪最差时的恶劣态度，我甚至觉得，\" \\\n",
    "              \"他是这个世界上唯一能用心感受我所经历的痛苦的人。但是，的确，\" \\\n",
    "              \"这个世界上是不存在感同身受的，我知道他努力过了，也曾经给过我很大的希望。现在，我想他也疲倦了。\" \\\n",
    "              \"写于2020年3月28日，于重庆\"\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(sample2_str)\n",
    "for entry in doc.ents:\n",
    "    print(entry.text, entry.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e06c73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  11\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:   42199 lr:  0.000000 avg.loss:  0.020128 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# 文本分类\n",
    "import fasttext\n",
    "\n",
    "classifier = fasttext.train_supervised(input='shuffle_data.txt', dim=100, epoch=1000, lr=0.1, wordNgrams=2, loss='softmax')\n",
    "classifier.save_model('classifier.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "054dad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__1,', '__label__2,'), array([0.68634456, 0.31367546]))\n",
      "(('__label__1,', '__label__2,'), array([0.76292688, 0.23709312]))\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict(\"有时候太难受的时候, 有点想一了百了\", k=2))\n",
    "print(classifier.predict(\"一了百了并不是解决问题的办法\", k=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd96ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本相似度计算\n",
    "import spacy\n",
    "\n",
    "def check_zh_doc_similarity():\n",
    "    nlp = spacy.load('zh_core_web_sm')\n",
    "    doc1 = nlp('你好吗?')\n",
    "    doc2 = nlp('你还好吗?')\n",
    "    doc3 = nlp('今天你还好吗?')\n",
    "    doc4 = nlp('你的身体今天还好吗?')\n",
    "    print(doc1.similarity(doc2))\n",
    "    print(doc2.similarity(doc3))\n",
    "    print(doc1.similarity(doc3))\n",
    "    print(doc1.similarity(doc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef99e7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7276577012401315\n",
      "0.9146431706372442\n",
      "0.628690517163414\n",
      "0.5599675731711398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/tgjmqxwd3mq_dmn00xtb25lw0000gn/T/ipykernel_27993/905372968.py:10: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc1.similarity(doc2))\n",
      "/var/folders/14/tgjmqxwd3mq_dmn00xtb25lw0000gn/T/ipykernel_27993/905372968.py:11: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc2.similarity(doc3))\n",
      "/var/folders/14/tgjmqxwd3mq_dmn00xtb25lw0000gn/T/ipykernel_27993/905372968.py:12: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc1.similarity(doc3))\n",
      "/var/folders/14/tgjmqxwd3mq_dmn00xtb25lw0000gn/T/ipykernel_27993/905372968.py:13: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc1.similarity(doc4))\n"
     ]
    }
   ],
   "source": [
    "check_zh_doc_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc9fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/14/tgjmqxwd3mq_dmn00xtb25lw0000gn/T/jieba.cache\n",
      "Loading model cost 0.298 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['美国', '输给', '中国女排', '输给', '郎平'], ['美国', '无缘', '四强', '主教练'], ['中国女排', '晋级', '世锦赛', '四强', '主教练', '郎平', '执教', '艺术'], ['买', 'MPV', 'SUV', '跑', '长途'], ['跑', '长途', 'SUV', '轿车', '差距'], ['家用', '轿车', '买']]\n"
     ]
    }
   ],
   "source": [
    "# 主题模型\n",
    "from gensim import corpora, models\n",
    "import jieba.posseg as jp, jieba\n",
    "\n",
    "# 文本集\n",
    "texts = [\n",
    "    '美国教练坦言，没输给中国女排，是输给了郎平',\n",
    "    '美国无缘四强，听听主教练的评价',\n",
    "    '中国女排晋级世锦赛四强，全面解析主教练郎平的执教艺术',\n",
    "    '为什么越来越多的人买MPV，而放弃SUV？跑一趟长途就知道了',\n",
    "    '跑了长途才知道，SUV和轿车之间的差距',\n",
    "    '家用的轿车买什么好']\n",
    "jieba.add_word('四强', 9, 'n')\n",
    "flags = ('n', 'nr', 'ns', 'nt', 'eng', 'v', 'd')  # 词性\n",
    "stopwords = ('没', '就', '知道', '是', '才', '听听', '坦言', '全面', '越来越', '评价', '放弃', '人')\n",
    "words_ls = []\n",
    "for text in texts:\n",
    "    words = [word.word for word in jp.cut(text) if word.flag in flags and word.word not in stopwords]\n",
    "    words_ls.append(words)\n",
    "print(words_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91fbcf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<19 unique tokens: ['中国女排', '美国', '输给', '郎平', '主教练']...>\n"
     ]
    }
   ],
   "source": [
    "# 去重，存到字典\n",
    "dictionary = corpora.Dictionary(words_ls)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45d9cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.079*\"轿车\" + 0.074*\"输给\" + 0.071*\"郎平\" + 0.070*\"中国女排\"')\n",
      "(1, '0.081*\"四强\" + 0.076*\"主教练\" + 0.072*\"美国\" + 0.066*\"跑\"')\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(words) for words in words_ls]\n",
    "# print(corpus)\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2)\n",
    "for topic in lda.print_topics(num_words=4):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24c3029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前面设置了num_topics = 2 所以这里有两个主题，显然第一个是汽车相关topic，第二个是体育相关topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ebd7ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[5.313778  , 0.6862094 ],\n",
      "       [0.6112416 , 4.3887486 ],\n",
      "       [7.9553366 , 1.0446328 ],\n",
      "       [1.14796   , 4.8520236 ],\n",
      "       [5.226868  , 0.77311647],\n",
      "       [3.376752  , 0.6232384 ]], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "# 主题推断\n",
    "print(lda.inference(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0685cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国女排将在郎平的率领下向世界女排三大赛的三连冠发起冲击\n",
      "\t主题0推断值2.38\n",
      "\t主题1推断值0.62\n"
     ]
    }
   ],
   "source": [
    "text5 = '中国女排将在郎平的率领下向世界女排三大赛的三连冠发起冲击'\n",
    "bow = dictionary.doc2bow([word.word for word in jp.cut(text5) if word.flag in flags and word.word not in stopwords])\n",
    "ndarray = lda.inference([bow])[0]\n",
    "print(text5)\n",
    "for e, value in enumerate(ndarray[0]):\n",
    "    print('\\t主题%d推断值%.2f' % (e, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf121b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本摘要\n",
    "# https://ai.baidu.com/tech/nlp_apply/news_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ea4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情感识别\n",
    "# https://ai.baidu.com/tech/nlp_apply/sentiment_classify"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
